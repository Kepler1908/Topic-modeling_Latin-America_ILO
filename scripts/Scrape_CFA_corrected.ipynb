{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import textwrap\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# Go to the website\n",
    "driver.get('https://www.ilo.org/dyn/normlex/en/f?p=1000:20061::FIND:NO:::') \n",
    "\n",
    "# Find elements by tag name\n",
    "rows = driver.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "# Open CSV writer\n",
    "with open('CFA_all_cases_final.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write header row\n",
    "    writer.writerow([\"case_and_country\", \"complaint_date\", \"organization\", \"short_description\", \"additional_texts\", \"href\"])\n",
    "\n",
    "    current_case = {}\n",
    "    for row in rows:\n",
    "        soup = BeautifulSoup(row.get_attribute('innerHTML'), 'html.parser')\n",
    "        td = soup.find('td', class_=\"apex_report_break\")\n",
    "        if td:\n",
    "            li_tags = td.find_all('li')  # Find all li tags under td, not just immediate children\n",
    "            for li_tag in li_tags:\n",
    "                a_tag = li_tag.find('a')\n",
    "                href = a_tag.get('href') if a_tag else None\n",
    "\n",
    "                if 'firstLevel' in li_tag.get('class', []):  # If it's a new case...\n",
    "                    if current_case:  # ... and there's a current case...\n",
    "                        # ... write its details to CSV and clear the dictionary\n",
    "                        writer.writerow(list(current_case.values()))\n",
    "                        current_case = {}\n",
    "                    a_text = a_tag.text if a_tag else None\n",
    "                    case_and_country, complaint_date = a_text.split(\" - Complaint date: \") if a_text and \" - Complaint date: \" in a_text else (a_text, None)\n",
    "                    organization = td.find('em').text if td.find('em') else None\n",
    "\n",
    "\n",
    "                    short_desc_elements = td.find_all('strong')# le code original : td.find_all('span', class_='secondLine')\n",
    "\n",
    "\n",
    "                    short_description =  short_desc_elements[1].text.strip() if len(short_desc_elements) > 1 else None\n",
    "                    current_case = {\"case_and_country\": case_and_country, \"complaint_date\": complaint_date, \"organization\": organization, \"short_description\": short_description, \"additional_texts\": [], \"href\": href}\n",
    "                    print(current_case)\n",
    "                else:  # If it's an associated document...\n",
    "                    if a_tag:  # If there's an <a> tag...\n",
    "                        current_case[\"additional_texts\"].append({\"text\": a_tag.text, \"href\": a_tag.get('href')})  # Update the associated documents in the dictionary\n",
    "\n",
    "    # Write the details of the last case\n",
    "    if current_case:\n",
    "        writer.writerow(list(current_case.values()))\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hrefs(additional_texts):\n",
    "    \"\"\"\n",
    "    function to extract correct href\n",
    "    \n",
    "    \"\"\"\n",
    "    if pd.isna(additional_texts) or additional_texts == '[]':\n",
    "        return None \n",
    "    try:\n",
    "        texts_list = ast.literal_eval(additional_texts)\n",
    "        hrefs = [text_dict['href'] for text_dict in texts_list]\n",
    "        return str(hrefs)\n",
    "    except:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv(\"D:/Chartes_cours/Memoire/data/CFA_all_cases_final.csv\") #File generated in the last bloc\n",
    "docs['hrefs'] = docs['additional_texts'].apply(extract_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected initial scraping script\n",
    "\n",
    "# Open the CSV file and load it into a DataFrame\n",
    "df = docs \n",
    "\n",
    "# Go to the website\n",
    "base_url = 'https://www.ilo.org/dyn/normlex/en/'\n",
    "\n",
    "# Create a requests session\n",
    "session = requests.Session()\n",
    "\n",
    "# Create a retry object\n",
    "retries = Retry(total=3, backoff_factor=5, status_forcelist=[ 502, 503, 504 ])\n",
    "\n",
    "# Mount it for both http and https usage\n",
    "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# Open a CSV file to write the results\n",
    "with open('scraped_texts_ALL_cleaned.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)  # Use QUOTE_ALL to ensure all fields are quoted\n",
    "    # Write the header\n",
    "    writer.writerow(['case', 'document_nr', 'document_name', 'document_href', 'text', 'log'])\n",
    "\n",
    "    # Loop over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"Processing row {idx + 1} out of {len(df)}\")\n",
    "        \n",
    "        case = row['case_and_country']\n",
    "        document_nr = row['complaint_date']\n",
    "        document_name = row['organization']\n",
    "        hrefs = row['hrefs'] #corrected coloumn names\n",
    "        \n",
    "        if pd.notna(hrefs):\n",
    "\n",
    "            #loop to make sure every single report can be scrapped\n",
    "            hrefs = ast.literal_eval(hrefs)\n",
    "            links = [base_url + href for href in hrefs]\n",
    "            for link in links:\n",
    "             \n",
    "\n",
    "             try:  \n",
    "                response = session.get(link, timeout=10)  # Add a timeout\n",
    "                log = f\"Scraping {link}, response status: {response.status_code}\"  # create log entry\n",
    "                \n",
    "                page_soup = BeautifulSoup(response.text, 'html.parser')  \n",
    "                text_box = page_soup.find('div', class_='textBoxConvention large')  \n",
    "                \n",
    "                if text_box is not None:  # Check if the element was found before trying to call get_text\n",
    "                    chunks = []\n",
    "                    for li in text_box.find_all('li'):\n",
    "                        number_tag = li.find('strong', class_='number')\n",
    "                        if number_tag is not None:\n",
    "                            number = number_tag.get_text(strip=True)\n",
    "                            text = li.get_text(strip=True).replace(number, '', 1)\n",
    "                            chunks.append((number, text))\n",
    "\n",
    "                    for chunk in chunks:\n",
    "                        chunk_text = \"\".join(chunk)\n",
    "                        chunk_text_split = textwrap.wrap(chunk_text, 500)\n",
    "\n",
    "                        for sub_chunk in chunk_text_split:\n",
    "                            # Write each sub-chunk as a separate row with the same case, document_nr, document_name, href, and log\n",
    "                            writer.writerow([case, document_nr, document_name, hrefs, sub_chunk, log])\n",
    "                else:\n",
    "                    log = f\"Could not find a 'div' with class 'textBoxConvention large' on page {link}\"  # update log entry\n",
    "                    # Write a row with the same case, document_nr, document_name, href, empty text, and log\n",
    "                    writer.writerow([case, document_nr, document_name, hrefs, '', log])\n",
    "             except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                time.sleep(10)  # Wait for 10 seconds before next attempt\n",
    "        else:\n",
    "            print(f\"Skipping row with missing 'document_href'.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
